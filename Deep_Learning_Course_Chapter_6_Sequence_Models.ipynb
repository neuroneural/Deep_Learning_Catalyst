{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrinal18/Deep_Learning_Catalyst/blob/main/Deep_Learning_Course_Chapter_6_Sequence_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Course Overview: \n",
        "\n",
        "1. Introduction to NLP\n",
        "2. word processing and bag of words\n",
        "3. Introduction to Recurrent Neural Network \n",
        "4. Introduction to Transformer Models and famous BERT.\n",
        "5. Introduction to Huggingface API\n",
        "\n",
        "\n",
        "HW: \n",
        "1. Read a research paper and produce a summary + psudo code of the paper.\n",
        "2. Building a LSTM for character level prediction\n",
        "3. Building Machine Translation using transformers and hugging face \n",
        "Note: Needs to be done using catalyst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Natural language Processing\n",
        "===========================================\n",
        "\n",
        "Text Processing is one of the most common task in many ML applications. Below are some examples of such applications.\n",
        "\n",
        "* Language Translation: Translation of a sentence from one language to another.\n",
        "*  Sentiment Analysis: To determine, from a text corpus, whether the  sentiment towards any topic or product etc. is positive, negative, or neutral.\n",
        "* Spam Filtering:  Detect unsolicited and unwanted email/messages.\n",
        "\n",
        "\n",
        "\n",
        "These applications deal with huge amount of text to perform classification or translation and involves a lot of work on the back end. Transforming text into something an algorithm can digest is a complicated process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduction to NLP\n",
        "===========================================\n",
        "Data comes in many different forms: time stamps, sensor readings, images, categorical labels, and so much more. But text is still some of the most valuable data out there for those who know how to use it.\n",
        "\n",
        "In this course about Natural Language Processing (NLP), you will use the leading NLP library to take on some of the most important tasks in working with text.\n",
        "\n",
        "By the end, you will be able to use NLP for:\n",
        "\n",
        "Basic text processing and pattern matching\n",
        "Building machine learning models with text\n",
        "Representing text with word embeddings that numerically capture the meaning of words and documents\n",
        "To get the most out of this course, you'll need some experience with machine learning. If you don't have experience with scikit-learn, check out Intro to Machine Learning and Intermediate Machine Learning to learn the fundamentals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4X9fAINJQnM"
      },
      "source": [
        "\n",
        "Word Processing and Bag of Words\n",
        "===========================================\n",
        "\n",
        "Word embeddings are dense vectors of real numbers, one per word in your vocabulary. In NLP, it is almost always the case that your features are words! But how should you represent a word in a computer? You could\n",
        "store its ascii character representation, but that only tells you what the word *is*, it doesn't say much about what it *means* (you might be able to derive its part of speech from its affixes, or properties from\n",
        "its capitalization, but not much). Even more, in what sense could you combine these representations? We often want dense outputs from our neural networks, where the inputs are $|V|$ dimensional, where\n",
        "$V$ is our vocabulary, but often the outputs are only a few dimensional (if we are only predicting a handful of labels, for instance). How do we get from a massive dimensional space to a smaller\n",
        "dimensional space?\n",
        "\n",
        "How about instead of ascii representations, we use a one-hot encoding? That is, we represent the word $w$ by \n",
        "$$\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align})$$\n",
        "\n",
        "where the 1 is in a location unique to $w$. Any other word will have a 1 in some other location, and a 0 everywhere else.\n",
        "\n",
        "There is an enormous drawback to this representation, besides just how huge it is. It basically treats all words as independent entities withno relation to each other. What we really want is some notion of\n",
        "*similarity* between words. Why? Let's see an example.\n",
        "\n",
        "Suppose we are building a language model. Suppose we have seen the\n",
        "sentences\n",
        "\n",
        "* The mathematician ran to the store.\n",
        "* The physicist ran to the store.\n",
        "* The mathematician solved the open problem.\n",
        "\n",
        "in our training data. Now suppose we get a new sentence never before seen in our training data:\n",
        "\n",
        "* The physicist solved the open problem.\n",
        "\n",
        "Our language model might do OK on this sentence, but wouldn't it be much better if we could use the following two facts:\n",
        "\n",
        "* We have seen  mathematician and physicist in the same role in a sentence. Somehow they have a semantic relation.\n",
        "* We have seen mathematician in the same role  in this new unseen sentence as we are now seeing physicist.\n",
        "\n",
        "and then infer that physicist is actually a good fit in the new unseen sentence? This is what we mean by a notion of similarity: we mean *semantic similarity*, not simply having similar orthographic\n",
        "representations. It is a technique to combat the sparsity of linguistic data, by connecting the dots between what we have seen and what we haven't. This example of course relies on a fundamental linguistic\n",
        "assumption: that words appearing in similar contexts are related to each other semantically. This is called the `distributional hypothesis <https://en.wikipedia.org/wiki/Distributional_semantics>`__.\n",
        "\n",
        "\n",
        "Getting Dense Word Embeddings \n",
        "How can we solve this problem? That is, how could we actually encode semantic similarity in words? Maybe we think up some semantic attributes. For example, we see that both mathematicians and physicists\n",
        "can run, so maybe we give these words a high score for the \"is able to run\" semantic attribute. Think of some other attributes, and imagine what you might score some common words on those attributes.\n",
        "\n",
        "If each attribute is a dimension, then we might give each word a vector, like this:\n",
        "\n",
        "$$\\begin{align}q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run}, \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\end{align}z$$\n",
        "\n",
        "$$\\begin{align}q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\end{align}$$\n",
        "\n",
        "Then we can get a measure of similarity between these words by doing:\n",
        "\n",
        "$$\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\end{align}$$\n",
        "\n",
        "Although it is more common to normalize by the lengths:\n",
        "\n",
        "$$\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}{\\| q_\\text{physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\end{align}$$\n",
        "\n",
        "Where $\\phi$ is the angle between the two vectors. That way, extremely similar words (words whose embeddings point in the same direction) will have similarity 1. Extremely dissimilar words should have similarity -1.\n",
        "\n",
        "You can think of the sparse one-hot vectors from the beginning of this section as a special case of these new vectors we have defined, where each word basically has similarity 0, and we gave each word some unique\n",
        "semantic attribute. These new vectors are *dense*, which is to say their entries are (typically) non-zero.\n",
        "\n",
        "But these new vectors are a big pain: you could think of thousands of different semantic attributes that might be relevant to determining similarity, and how on earth would you set the values of the different\n",
        "attributes? Central to the idea of deep learning is that the neural network learns representations of the features, rather than requiring the programmer to design them herself. So why not just let the word\n",
        "embeddings be parameters in our model, and then be updated during training? This is exactly what we will do. We will have some *latent semantic attributes* that the network can, in principle, learn. Note\n",
        "that the word embeddings will probably not be interpretable. That is, although with our hand-crafted vectors above we can see that mathematicians and physicists are similar in that they both like coffee,\n",
        "if we allow a neural network to learn the embeddings and see that both mathematicians and physicists have a large value in the second dimension, it is not clear what that means. They are similar in some\n",
        "latent semantic dimension, but this probably has no interpretation to us.\n",
        "\n",
        "\n",
        "In summary, **word embeddings are a representation of the *semantics* ofa word, efficiently encoding semantic information that might be relevant to the task at hand**. You can embed other things too: part of speech tags, parse trees, anything! The idea of feature embeddings is central to the field.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRTjnjMkJ81q"
      },
      "source": [
        "An Example: N-Gram Language Modeling\n",
        "\n",
        "Recall that in an n-gram language model, given a sequence of words  ùë§ , we want to compute\n",
        "\n",
        "ùëÉ(ùë§ùëñ|ùë§ùëñ‚àí1,ùë§ùëñ‚àí2,‚Ä¶,ùë§ùëñ‚àíùëõ+1) \n",
        "\n",
        "Where  ùë§ùëñ  is the ith word of the sequence.\n",
        "\n",
        "In this example, we will compute the loss function on some training examples and update the parameters with backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Spacy\n",
        "\n",
        "Before we delve on using Natural language for using deep learning for sequence modling, we will first look at how to use tokens and word embeddings first\n",
        "\n",
        "## NLTK and Spacy:\n",
        "source: https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/\n",
        "\n",
        "NLTK and spacy both are tools to be used for any natural language pipelines, both has more it's advantages and disadvantages \n",
        "\n",
        "to install spacy for your machine, please refer to this site:  https://spacy.io/usage\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. ‚ÄúI can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn‚Äôt \"\n",
        "        \"worth talking to,‚Äù said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Natural Language Models\n",
        "\n",
        "Basically we input the text into a neural network, the neural network will map all this context onto a vector. This vector represents the next word and we have some big word embedding matrix. The word embedding matrix contains a vector for every possible word the model can output. We then compute similarity by dot product of the context vector and each of the word vectors. We‚Äôll get a likelihood of predicting the next word, then train this model by maximum likelihood. The key detail here is that we don‚Äôt deal with words directly, but we deal with things called sub-words or characters.\n",
        "\n",
        "\n",
        "$$p(x_0 \\| x_0, ..., n-1) = softmax(E(f(x_0 \\| x_0, ..., n-1)))$$\n",
        "\n",
        "![title](figures/fig1.jpeg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convolutional Language Models\n",
        "The first neural language model\n",
        "Embed each word as a vector, which is a lookup table to the embedding matrix, so the word will get the same vector no matter what context it appears in Apply same feed forward network at each time step Unfortunately, fixed length history means it can only condition on bounded context These models do have the upside of being very fast \n",
        "\n",
        "![title](figures/cnn_language_model.jpeg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recurrent Language Models\n",
        "### The most popular approach until a couple years ago. \n",
        "\n",
        "Conceptually straightforward: every time step we maintain some state (received from the previous time step), which represents what we‚Äôve read so far. This is  combined with current word being read and used at later state. Then we repeat this process for as many time steps as we need. \n",
        "\n",
        "Uses unbounded context: in principle the title of a book would affect the hidden states of last word of the book. \n",
        "\n",
        "Disadvantages:\n",
        "* The whole history of the document reading is compressed into fixed-size.\n",
        "* vector at each time step, which is the bottleneck of this model.\n",
        "* Gradients tend to vanish with long contexts.\n",
        "* Not possible to parallelize over time-steps, so slow training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Install catalyst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Text preprocessing is the most important part of NLP. \n",
        "\n",
        "In comparison, an image is usually reshaped and normalized in a preprocessing pipeline. But a text is different. A text consists of words(or tokens), that has a different probability to be written. Words are arrays of characters, and different arrays can be related to one word(E.g. \"it\" and \"It\" or \"–ò–º—è\" and –ò–º–µ–Ω–∏\" is one word, but different word form.). \n",
        "\n",
        "That's why texts should be normalized and tokenized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing the Data\n",
        "\n",
        "Text preprocessing is the most important part of NLP. In comparison, an image is usually reshaped and normalized in a preprocessing pipeline. But a text is different. A text consists of words(or tokens), that has a different probability to be written. Words are arrays of characters, and different arrays can be related to one word(E.g. \"it\" and \"It\" or \"–ò–º—è\" and \"–ò–º–µ–Ω–∏\" is one word, but different word form.). That's why texts should be normalized and tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Implementing RNN from scratch. \n",
        "\n",
        "\"\"\"\n",
        "We will be building and training a basic character-level RNN to classify words. This tutorial, along with the following two, show how to do preprocess data for NLP modeling ‚Äúfrom scratch‚Äù, in particular not using many of the convenience functions of torchtext, so you can see how preprocessing for NLP modeling works at a low level.\n",
        "\n",
        "A character-level RNN reads words as a series of characters - outputting a prediction and ‚Äúhidden state‚Äù at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to.\n",
        "\n",
        "Specifically, we‚Äôll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling:\n",
        "\n",
        "python predict.py Hinton\n",
        "(-0.47) Scottish\n",
        "(-1.52) English\n",
        "(-3.57) Irish\n",
        "\n",
        "python predict.py Schmidhuber\n",
        "(-0.19) German\n",
        "(-2.48) Czech\n",
        "(-2.68) Dutch\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#Implementing RNN from scratch using catalyst\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNxC4eW8/OjN7REGdlxJIDY",
      "include_colab_link": true,
      "name": "Deep Learning Course_Chapter_6: Sequence Models",
      "private_outputs": true,
      "provenance": []
    },
    "interpreter": {
      "hash": "f96068154b5bc6e6548f49214f975f9d1513bfaede5d9ab8bc35c7389c0dcf91"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
