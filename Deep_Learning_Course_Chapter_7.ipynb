{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrinal18/Deep_Learning_Catalyst/blob/main/Deep_Learning_Course_Chapter_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter <> : Introduction to Deep Learning in Images and Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TO Dos\n",
        "\n",
        "1. Add images for explanation \n",
        "2. add content for Real time detection \n",
        "3. new models for image recognition introduction\n",
        "4. add HW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwHf6MhoOw6r"
      },
      "source": [
        "In this chapter we will be looking into the capability of deep learning to classify images and videos for different datasets. Image Recognition is the task which made us realise the power for deep learning.\n",
        "\n",
        "For starter, we will give the introduction of Convolution Neural Network and then move to a simple dataset which will allow us to look into recognition of images by deep learning more clearly.\n",
        "\n",
        "For using image processing we have four basic steps:\n",
        "\n",
        "1. Downlaod the image\n",
        "2. split the image into train and validation sets \n",
        "3. Transform your image/clean your datasets \n",
        "4. Add your dataset split into a Dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn1xYcuiFO8G"
      },
      "outputs": [],
      "source": [
        "#we use four basic steps in any image processing involving deep learning\n",
        "\"\"\"\n",
        "1. Download images\n",
        "2. Split them into train and validation datasets\n",
        "3. Add extra image transforms for splits (these are highly subjective)\n",
        "4. Wrap each dataset split into a DataLoader\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn\n",
        "!pip install python-requests\n",
        "from catalyst.contrib.models import SequentialNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxZ_EaKRPpZ1"
      },
      "source": [
        "Before moving to the code, we will study about a brief overview of CNN: \n",
        "\n",
        "Convolutional Networks work by moving small filters across the input image. This means the filters are re-used for recognizing patterns throughout the entire input image. This makes the Convolutional Networks much more powerful than Fully-Connected networks with the same number of variables. This in turn makes the Convolutional Networks faster to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EESVeCUdQqw8"
      },
      "source": [
        "The input image is processed in the first convolutional layer using the filter-weights. This results in 16 new images, one for each filter in the convolutional layer. The images are also down-sampled so the image resolution is decreased from 28x28 to 14x14.\n",
        "\n",
        "These 16 smaller images are then processed in the second convolutional layer. We need filter-weights for each of these 16 channels, and we need filter-weights for each output channel of this layer. There are 36 output channels so there are a total of 16 x 36 = 576 filters in the second convolutional layer. The resulting images are down-sampled again to 7x7 pixels.\n",
        "\n",
        "The output of the second convolutional layer is 36 images of 7x7 pixels each. These are then flattened to a single vector of length 7 x 7 x 36 = 1764, which is used as the input to a fully-connected layer with 128 neurons (or elements). This feeds into another fully-connected layer with 10 neurons, one for each of the classes, which is used to determine the class of the image, that is, which number is depicted in the image.\n",
        "\n",
        "The convolutional filters are initially chosen at random, so the classification is done randomly. The error between the predicted and true class of the input image is measured as the so-called cross-entropy. The optimizer then automatically propagates this error back through the Convolutional Network using the chain-rule of differentiation and updates the filter-weights so as to improve the classification error. This is done iteratively thousands of times until the classification error is sufficiently low.\n",
        "\n",
        "These particular filter-weights and intermediate images are the results of one optimization run and may look different if you re-run this Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB1SxDHRQzY8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "from catalyst import dl\n",
        "from catalyst.utils import metrics\n",
        "from catalyst.contrib.nn import Flatten\n",
        "from catalyst.contrib.models import SequentialNet\n",
        "\n",
        "\n",
        "# the data\n",
        "train_dataset = MNIST(\"./mnist\", train=True, download=True, transform=ToTensor())\n",
        "valid_dataset = MNIST(\"./mnist\", train=False, download=True, transform=ToTensor())\n",
        "train_loader = DataLoader(train_dataset, batch_size=32)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
        "\n",
        "# the model\n",
        "model = nn.Sequential(\n",
        "    Flatten(),\n",
        "    SequentialNet(\n",
        "        hiddens=[28 * 28, 128, 128, 10], \n",
        "        layer_fn=nn.Linear, \n",
        "        activation_fn=nn.ReLU\n",
        "    )\n",
        ")\n",
        "\n",
        "# the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# the loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# runner          \n",
        "runner = dl.SupervisedRunner()\n",
        "# model training\n",
        "runner.train(\n",
        "    loaders={\"train\": train_loader, \"valid\": valid_loader},\n",
        "    model=model, criterion=criterion, optimizer=optimizer,\n",
        "    num_epochs=1, logdir=\"./logs\", verbose=True,\n",
        "    callbacks=[dl.AccuracyCallback(num_classes=10)],\n",
        "    load_best_on_end=True,\n",
        ")\n",
        "# model inference\n",
        "for prediction in runner.predict_loader(loader=valid_loader):\n",
        "    assert prediction.detach().cpu().numpy().shape[-1] == 10\n",
        "# model tracing\n",
        "traced_model = runner.trace(loader=valid_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e73v2vTFOXl5"
      },
      "source": [
        "In the previous lesson you saw how to use a CNN to make your recognition of the handwriting digits more efficient. In this lesson you'll take that to the next level, recognizing real images of Cats and Dogs in order to classify an incoming image as one or the other. In particular the handwriting recognition made your life a little easier by having all the images be the same size and shape, and they were all monochrome color. Real-world images aren't like that -- they're in different shapes, aspect ratios etc, and they're usually in color!\n",
        "\n",
        "So, as part of the task you need to process your data -- not least resizing it to be uniform in shape. \n",
        "\n",
        "You'll follow these steps:\n",
        "\n",
        "1.   Explore the Example Data of Cats and Dogs\n",
        "2.   Build and Train a Neural Network to recognize the difference between the two\n",
        "3.   Evaluate the Training and Validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMjkHh6JOcPj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Implement a classification of cats and dogs using catalyst without any arguments.\n",
        "2. extract dataset from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import catalyst\n",
        "from catalyst.dl import SupervisedRunner\n",
        "from catalyst.dl.callbacks import AccuracyCallback\n",
        "\n",
        "# 1. prepare data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder('../data/cat_dog/train', transform=transform)\n",
        "valid_data = datasets.ImageFolder('../data/cat_dog/valid', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "loaders = {\"train\": train_loader, \"valid\": valid_loader}\n",
        "\n",
        "# 2. prepare model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 53 * 53)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# 3. prepare optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 4. prepare callbacks\n",
        "callbacks = [AccuracyCallback(num_classes=2, accuracy_args=[1])]\n",
        "\n",
        "# 5. train model\n",
        "runner = SupervisedRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    callbacks=callbacks,\n",
        "    logdir=\"./logs\",\n",
        "    num_epochs=10,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2P205dDrkuQ"
      },
      "source": [
        "## plotting convolutional weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwYCT718T8OM"
      },
      "outputs": [],
      "source": [
        "def plot_conv_weights(weights, input_channel=0):\n",
        "    # Assume weights are ops for 4-dim variables\n",
        "    # e.g. weights_conv1 or weights_conv2.\n",
        "    \n",
        "    # A feed-dict is not necessary because nothing is calculated.\n",
        "    w = session.run(weights)\n",
        "\n",
        "    # Get the lowest and highest values for the weights.\n",
        "    # This is used to correct the colour intensity across\n",
        "    # the images so they can be compared with each other.\n",
        "    w_min = np.min(w)\n",
        "    w_max = np.max(w)\n",
        "\n",
        "    # Number of filters used in the conv. layer.\n",
        "    num_filters = w.shape[3]\n",
        "\n",
        "    # Number of grids to plot.\n",
        "    # Rounded-up, square-root of the number of filters.\n",
        "    num_grids = math.ceil(math.sqrt(num_filters))\n",
        "    \n",
        "    # Create figure with a grid of sub-plots.\n",
        "    fig, axes = plt.subplots(num_grids, num_grids)\n",
        "\n",
        "    # Plot all the filter-weights.\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Only plot the valid filter-weights.\n",
        "        if i<num_filters:\n",
        "            # Get the weights for the i'th filter of the input channel.\n",
        "            # See new_conv_layer() for details on the format\n",
        "            # of this 4-dim tensor.\n",
        "            img = w[:, :, input_channel, i]\n",
        "\n",
        "            # Plot image.\n",
        "            ax.imshow(img, vmin=w_min, vmax=w_max,\n",
        "                      interpolation='nearest', cmap='seismic')\n",
        "        \n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcVJFvmhry-r"
      },
      "outputs": [],
      "source": [
        "plot_conv_weights(weights=weights_conv1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmaRmRRbrzVO"
      },
      "outputs": [],
      "source": [
        "plot_conv_weights(weights=weights_conv2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69jfSHonr6QT"
      },
      "source": [
        "Homework:\n",
        "These are a few suggestions for exercises that may help improve your skills with Catalyst. It is important to get hands-on experience with Catalyst in order to learn how to use it properly.\n",
        "\n",
        "You may want to backup this Notebook before making any changes.\n",
        "\n",
        "Do you get the exact same results if you run the Notebook multiple times without changing any parameters? What are the sources of randomness?\n",
        "1. Run another 10,000 optimization iterations. Are the results better?\n",
        "2. Change the learning-rate for the optimizer.\n",
        "3. Change the configuration of the layers, such as the number of convolutional filters, the size of those filters, the number of neurons in the fully-connected layer, etc.\n",
        "4. Add a so-called drop-out layer after the fully-connected layer. Note that the drop-out probability should be zero when calculating the classification accuracy, so you will need a placeholder variable for this probability.\n",
        "5. Change the order of ReLU and max-pooling in the convolutional layer. Does it calculate the same thing? What is the fastest way of computing it? How many calculations are saved? Does it also work for Sigmoid-functions and average-pooling?\n",
        "6. Add one or more convolutional and fully-connected layers. Does it help performance?\n",
        "7. What is the smallest possible configuration that still gives good results?\n",
        "8. Try using ReLU in the last fully-connected layer. Does the performance change? Why?\n",
        "9. Try not using pooling in the convolutional layers. Does it change the classification accuracy and training time?\n",
        "10. Try using a 2x2 stride in the convolution instead of max-pooling? What is the difference?\n",
        "Remake the program yourself without looking too much at this source-code.\n",
        "11. Explain to a friend how the program works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL1GOOElsVSt"
      },
      "source": [
        "### Image Augmentation\n",
        "\n",
        "Image Augmentation is a very simple, but very powerful tool to help you avoid overfitting your data. The concept is very simple though: If you have limited data, then the chances of you having data to match potential future predictions is also limited, and logically, the less data you have, the less chance you have of getting accurate predictions for data that your model hasn't yet seen. To put it simply, if you are training a model to spot cats, and your model has never seen what a cat looks like when lying down, it might not recognize that in future.\n",
        "\n",
        "Augmentation simply amends your images on-the-fly while training using transforms like rotation. So, it could 'simulate' an image of a cat lying down by rotating a 'standing' cat by 90 degrees. As such you get a cheap way of extending your dataset beyond what you have already. \n",
        "\n",
        "To learn more about Augmentation, and the available transforms, check out https://github.com/keras-team/keras-preprocessing -- and note that it's referred to as preprocessing for a very powerful reason: that it doesn't require you to edit your raw images, nor does it amend them for you on-disk. It does it in-memory as it's performing the training, allowing you to experiment without impacting your dataset. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GatCnEy6usm9"
      },
      "source": [
        "### Transfer Learning\n",
        "\n",
        "\n",
        "What if you could take an existing model that's trained on far more data, and use the features that that model learned? That's the concept of transfer learning, and we'll explore that in this lesson. So for example, if you visualize your model like this with a series of convolutional layers before dense layer leads your output layer, you feed your data into the top layer, the network learns the convolutions that identify the features in your data and all that. But consider somebody else's model, perhaps one that's far more sophisticated than yours, trained on a lot more data. They have convolutional layers and they're here intact with features that have already been learned. So you can lock them instead of retraining them on your data, and have those just extract the features from your data using the convolutions that they've already learned. Then you can take a model that has been trained on a very large datasets and use the convolutions that it learned when classifying its data. \n",
        "\n",
        "If you recall how convolutions are created and used to identify particular features, and the journey of a feature through the network, it makes sense to just use those, and then retrain the dense layers from that model with your data. Of course, well, it's typical that you might lock all the convolutions. You don't have to. You can choose to retrain some of the lower ones too because they may be too specialized for the images at hand. It takes some trial and error to discover the right combination. \n",
        "\n",
        "So let's take a well-trained state of the art model. There's one called Inception, which you can learn more about at his site. This has been pre-trained on a dataset from ImageNet, which has 1.4 million images in a 1000 different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm-3ix3wsMHO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPDCL1DjipH9ZSdLgyghzRl",
      "include_colab_link": true,
      "name": "Deep Learning Course_Chapter_7",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
