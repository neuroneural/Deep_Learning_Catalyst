{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing what ConvNets learn\n",
    "\n",
    "\n",
    "There are many approces for visualising convolutional networks. In this notebook we will look at few ways to approch them:\n",
    "\n",
    "* Layer Activation\n",
    "* Conv/FC Filters\n",
    "* Embedding the codes with t-SNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Layer Activation\n",
    "\n",
    "Layer Activations. The most straight-forward visualization technique is to show the activations of the network during the forward pass. For ReLU networks, the activations usually start out looking relatively blobby and dense, but as the training progresses the activations usually become more sparse and localized. One dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs, which can indicate dead filters, and can be a symptom of high learning rates.\n",
    "\n",
    "\n",
    "![https://cs231n.github.io/assets/cnnvis/act1.jpeg](https://cs231n.github.io/assets/cnnvis/act1.jpeg)![https://cs231n.github.io/assets/cnnvis/act2.jpeg](https://cs231n.github.io/assets/cnnvis/act2.jpeg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVGG = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelVGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv.imread(\"/content/hymenoptera_data/val/bees/1297972485_33266a18d9.jpg\")\n",
    "img=cv.cvtColor(img,cv.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "img=np.array(img)\n",
    "img=transform(img)\n",
    "img=img.unsqueeze(0)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_layers=0\n",
    "conv_layers=[]\n",
    "\n",
    "model_children=list(modelVGG.children())\n",
    "\n",
    "for child in model_children:\n",
    "  if type(child)==nn.Conv2d:\n",
    "    no_of_layers+=1\n",
    "    conv_layers.append(child)\n",
    "  elif type(child)==nn.Sequential:\n",
    "    for layer in child.children():\n",
    "      if type(layer)==nn.Conv2d:\n",
    "        no_of_layers+=1\n",
    "        conv_layers.append(layer)\n",
    "print(no_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [conv_layers[0](img)]\n",
    "for i in range(1, len(conv_layers)):\n",
    "    results.append(conv_layers[i](results[-1]))\n",
    "outputs = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize 8 features map from each layer \n",
    "for num_layer in range(len(outputs)):\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    layer_viz = outputs[num_layer][0, :, :, :]\n",
    "    layer_viz = layer_viz.data\n",
    "    print(\"Layer \",num_layer+1)\n",
    "    for i, filter in enumerate(layer_viz):\n",
    "        if i == 16: \n",
    "            break\n",
    "        plt.subplot(2, 8, i + 1)\n",
    "        plt.imshow(filter, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv/FC Filters. \n",
    "The second common strategy is to visualize the weights. These are usually most interpretable on the first CONV layer which is looking directly at the raw pixel data, but it is possible to also show the filter weights deeper in the network. The weights are useful to visualize because well-trained networks usually display nice and smooth filters without any noisy patterns. Noisy patterns can be an indicator of a network that hasnâ€™t been trained for long enough, or possibly a very low regularization strength that may have led to overfitting.\n",
    "\n",
    "![https://cs231n.github.io/assets/cnnvis/filt1.jpeg](https://cs231n.github.io/assets/cnnvis/filt1.jpeg)![https://cs231n.github.io/assets/cnnvis/filt2.jpeg](https://cs231n.github.io/assets/cnnvis/filt2.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the first and second layer of convolutional layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the codes with t-SNE\n",
    "(WE WILL STUDY THIS LATER)\n",
    "\n",
    "ConvNets can be interpreted as gradually transforming the images into a representation in which the classes are separable by a linear classifier. We can get a rough idea about the topology of this space by embedding images into two dimensions so that their low-dimensional representation has approximately equal distances than their high-dimensional representation. There are many embedding methods that have been developed with the intuition of embedding high-dimensional vectors in a low-dimensional space while preserving the pairwise distances of the points. Among these, t-SNE is one of the best-known methods that consistently produces visually-pleasing results.\n",
    "\n",
    "To produce an embedding, we can take a set of images and use the ConvNet to extract the CNN codes (e.g. in AlexNet the 4096-dimensional vector right before the classifier, and crucially, including the ReLU non-linearity). We can then plug these into t-SNE and get 2-dimensional vector for each image. The corresponding images can them be visualized in a grid:\n",
    "\n",
    "![https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k.jpg](https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To produce an embedding, we can take a set of images and use the ConvNet to extract the CNN codes (e.g. in AlexNet the 4096-dimensional vector right before the classifier, and crucially, including the ReLU non-linearity). We can then plug these into t-SNE and get 2-dimensional vector for each image. The corresponding images can them be visualized in a grid\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "#TODO: visualize code using t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
