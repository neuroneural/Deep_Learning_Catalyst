{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isyvChpp3x1Z"
      },
      "source": [
        "# The Linear Classifier\n",
        "\n",
        "\n",
        "To illustrate the workflow for training a deep learning model in a supervised manner, this notebook will walk you through the simple case of training a linear classifier to recognize images of cats and dogs. While deep learning might seem intimidating, dont worry. Its conceptual underpinnings are rooted in linear algebra and calculus - if you can perform matrix multiplication and take derivatives you can understand what is happening in a deep learning workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWC5tKyP3x1e"
      },
      "source": [
        "Some code cells will be marked with \n",
        "```\n",
        "##########################\n",
        "######## To Do ###########\n",
        "##########################\n",
        "```\n",
        "\n",
        "This indicates that you are being asked to write a piece of code to complete the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmSqc1mI3x1f"
      },
      "source": [
        "## Load packages\n",
        "In this cell, we load the python packages we need for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_DwKG_Gi3x1f"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import skimage\n",
        "import sklearn.model_selection\n",
        "import skimage.color\n",
        "import skimage.transform\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ2jTjsd3x1g"
      },
      "source": [
        "## The supervised machine learning workflow\n",
        "Recall from class the conceptual workflow for a supervised machine learning project. \n",
        "- First, we create a <em>training dataset</em>, a paired collection of raw data and labels where the labels contain information about the \"insight\" we wish to extract from the raw data. \n",
        "- Once we have training data, we can then use it to train a <em>model</em>. The model is a mathematical black box - it takes in data and transforms it into an output. The model has some parameters that we can adjust to change how it performs this mapping. \n",
        "- Adjusting these parameters to produce outputs that we want is called training the model. To do this we need two things. First, we need a notion of what we want the output to look like. This notion is captured by a <em>loss function</em>, which compares model outputs and labels and produces a score telling us if the model did a \"good\" job or not on our given task. By convention, low values of the loss function's output (e.g. the loss) correspond to good performance and high values to bad performance. We also need an <em>optimization algorithm</em>, which is a set of rules for how to adjust the model parameters to reduce the loss\n",
        "- Using the training data, loss function, and optimization algorithm, we can then train the model \n",
        "- Once the model is trained, we need to evaluate its performance to see how well it performs and what kinds of mistakes it makes. We can also perform this kind of monitoring during training (this is actually a standard practice).\n",
        "\n",
        "Because this workflow defines the lifecycle of most machine learning projects, this notebook is structured to go over each of these steps while constructing a linear classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbIDLvJ23x1g"
      },
      "source": [
        "## Create training data\n",
        "The starting point of every machine learning project is data. In this case, we will start with a collection of RGB images of cats and dogs. Each image is a multi-dimensional array with size (128, 128, 1) - the first two dimensions are spatial while the last is a channel dimension (one channel because it is a grey scale image - for an RGB image there would be 3 channels). The dataset that we are working with is a subset of [Kaggle's Dogs vs. Cats dataset](https://www.kaggle.com/c/dogs-vs-cats/overview). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H7Zn7fvCC8ML",
        "tags": [
          "remove-output"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-11-07 01:36:49--  https://storage.googleapis.com/datasets-spring2021/cats-and-dogs-bw.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4023:1004::80, 2607:f8b0:4023:1000::80, 2607:f8b0:4023:1002::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4023:1004::80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 717779623 (685M) [application/octet-stream]\n",
            "Saving to: ‘cats-and-dogs-bw.npz’\n",
            "\n",
            "cats-and-dogs-bw.np 100%[===================>] 684.53M  41.5MB/s    in 16s     \n",
            "\n",
            "2021-11-07 01:37:06 (42.1 MB/s) - ‘cats-and-dogs-bw.npz’ saved [717779623/717779623]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/datasets-spring2021/cats-and-dogs-bw.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YCRDjFTu3x1h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8192, 128, 128, 1) (8192,)\n"
          ]
        }
      ],
      "source": [
        "# Load data from the downloaded npz file\n",
        "with np.load('cats-and-dogs-bw.npz') as f:\n",
        "    X = f['X']\n",
        "    y = f['y']\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG4mmJ173x1h"
      },
      "source": [
        "In the previous cell, you probably observed that there are 4 dimensions rather than the 3 you might have been expecting. This is because while each image is (128, 128, 1), the full dataset has many images. The different images are stacked along the first dimension. The full size of the training images is (# images, 128, 128, 1) - the first dimension is often called the batch dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5RIxP_hq3x1i"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "######## To Do ###########\n",
        "##########################\n",
        "\n",
        "# Use matplotlib to visualze several images randomly drawn from the dataset\n",
        "# For each image, set the title to be the y label for the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D_YhVir3x1i"
      },
      "source": [
        "For this exercise, we will want to flatten the training data into a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J3MJPnh-3x1j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8192, 16384, 1)\n"
          ]
        }
      ],
      "source": [
        "# Flatten the images into vectors of size (# images, 16384, 1)\n",
        "X = np.reshape(X, (-1, 128*128, 1))\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2yrGjOL3x1j"
      },
      "source": [
        "### Split the training dataset into training, validation, and testing datasets\n",
        "How do we know how well our model is doing? A common practice to evaluate models is to evaluate them on splits of the original training dataset. Splitting the data is important, because we want to see how models perform on data that wasn't used to train them. This splitting practice usually produces 3 splits.\n",
        "- The <em>training</em> dataset used to train the model\n",
        "- A <em>validation </em> dataset used to evaluate the model during training. \n",
        "- A held out <em>testing</em> dataset used to evaluate the final trained version of the model\n",
        "While there is no hard and fast rule, 80%, 10%, 10% splits are a reasonable starting point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hLQUiSoj3x1j"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training, validation, and testing splits\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fhlPKpe3x1j"
      },
      "source": [
        "## The linear classifier\n",
        "The linear classifier produces class scores that are a linear function of the pixel values. Mathematically, this can be written as $\\vec{y} = W \\vec{x}$, where $\\vec{y}$ is the vector of class scores, $W$ is a matrix of weights and $\\vec{x}$ is the image vector. The shape of the weights matrix is determined by the number of classes and the length of the image vector. In this case $W$ is 2 by 4096. Our learning task is to find a set of weights that maximize our performance on our classification task. We will solve this task by doing the following steps\n",
        "- Randomly initializing a set of weights\n",
        "- Defining a loss function that measures our performance on the classification task\n",
        "- Use stochastic gradient descent to find \"optimal\" weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0LQ99KR3x1j"
      },
      "source": [
        "### Create the matrix of weights\n",
        "Properly initializing weights is essential for getting deep learning methods to work correctly. The two most common initialization methods you'll see in this class are [glorot uniform (also known as Xavier) initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi]) and [he initialization](http://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html) - both papers are worth reading. For this exercise, we will randomly initialize weights by using glorot uniform initialization. In this initialization method, we sample our weights according to the formula \n",
        "\\begin{equation}\n",
        "W_{ij} \\sim U\\left[ -\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}} \\right],\n",
        "\\end{equation}\n",
        "where $n$ is the number of columns in the weight matrix (4096 in our case).\n",
        "\n",
        "Lets create the linear classifier using object oriented programming, which will help with organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qL46rSt_gRJz",
        "tags": [
          "remove-output"
        ]
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1222711640.py, line 15)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/var/folders/zx/7f5hrldd0f3f7d6cvmhrtjt00000gn/T/ipykernel_6501/1222711640.py\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    self.W = # Add weights matrix here\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "class LinearClassifier(object):\n",
        "    def __init__(self, image_size=16384):\n",
        "        self.image_size=image_size\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "        \n",
        "    def _initialize_weights(self):\n",
        "        \n",
        "        ##########################\n",
        "        ######## To Do ###########\n",
        "        ##########################\n",
        "        \n",
        "        # Randomly initialize the weights matrix acccording to the glorot uniform initialization\n",
        "        self.W = np.random.uniform(-0.1, 0.1, (self.image_size, 1))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yeA5geI3x1k"
      },
      "source": [
        "### Apply the softmax transform to complete the model outputs\n",
        "Our LinearClassifier class needs a method to perform predictions - which in our case is performing matrix multiplication and then applying the softmax transform. Recall from class that the softmax transform is given by\n",
        "\\begin{equation}\n",
        "softmax(y_i) = \\frac{e^{y_i}}{\\sum_j e^{y_j}}\n",
        "\\end{equation}\n",
        "and provides a convenient way to convert our class scores into probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YadGdARMgRJ0",
        "tags": [
          "remove-output"
        ]
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "######## To Do ###########\n",
        "##########################\n",
        "\n",
        "# Complete the predict function below to predict a label y from an input X\n",
        "# Pay careful attention to the shape of your data at each step\n",
        "\n",
        "def predict(self, X, epsilon=1e-5):\n",
        "    # matrix multiplication\n",
        "    y = np.matmul(X, self.W)\n",
        "\n",
        "    # Apply softmax\n",
        "    y = np.exp(y) / np.sum(np.exp(y), axis=1, keepdims=True)\n",
        "    return y\n",
        "\n",
        "# Assign methods to class\n",
        "setattr(LinearClassifier, 'predict', predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9l7Zum93x1l"
      },
      "source": [
        "Now lets see what happens when we try to predict the class of images in our training dataset using randomly initialized weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tgJymU33x1l"
      },
      "outputs": [],
      "source": [
        "lc = LinearClassifier()\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(20,20))\n",
        "for i in range(8):\n",
        "    \n",
        "    # Get an example image\n",
        "    X_sample = X[[i],...]\n",
        "    \n",
        "    # Reshape flattened vector to image\n",
        "    X_reshape = np.reshape(X_sample, (128,128))\n",
        "    \n",
        "    # Predict the label\n",
        "    y_pred = lc.predict(X_sample)\n",
        "    \n",
        "    # Display results\n",
        "    axes.flatten()[i].imshow(X_reshape, cmap='gray')\n",
        "    axes.flatten()[i].set_title('Label ' + str(y[i]) +', Prediction ' + str(y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iKRfu3J3x1l"
      },
      "source": [
        "What do you notice about the initial results of the model? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv4Rc_xS3x1l"
      },
      "source": [
        "## Stochastic gradient descent\n",
        "To train this model, we will use stochastic gradient descent. In its simplest version, this algorithm consists of the following steps:\n",
        "- Select several images from the training dataset at random\n",
        "- Compute the gradient of the loss function with respect to the weights, given the selected images\n",
        "- Update the weights using the update rule $\\Delta W_{ij} \\rightarrow \\Delta W_{ij} - lr\\frac{\\partial loss}{\\partial W_{ij}}$\n",
        "\n",
        "Recall that the origin of this update rule is from multivariable calculus - the gradient tells us the direction in which the loss function increases the most. So to minimize the loss function we move in the opposite direction of the gradient.\n",
        "\n",
        "Also recall from the course notes that for this problem we can compute the gradient analytically. The gradient is given by\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial loss}{\\partial W_{ij}} = \\left(p_i - 1(i \\mbox{ is correct}) \\right)x_j,\n",
        "\\end{equation}\n",
        "where $1$ is an indicator function that is 1 if the statement inside the parentheses is true and 0 if it is false."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXHGyfz33x1l",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def grad(self, X, y):\n",
        "    # Get class probabilities\n",
        "    p = self.predict(X)\n",
        "    \n",
        "    # Compute class 0 gradients\n",
        "    temp_0 = np.expand_dims(p[...,0] - (1-y), axis=-1)\n",
        "    grad_0 = temp_0 * X[...,0]\n",
        "\n",
        "    # Compute class 1 gradients\n",
        "    temp_1 = np.expand_dims(p[...,1] - y, axis=-1)\n",
        "    grad_1 =  temp_1 * X[...,0]\n",
        "    \n",
        "    gradient = np.stack([grad_0, grad_1], axis=1)\n",
        "    \n",
        "    return gradient\n",
        "    \n",
        "def loss(self, X, y_true):\n",
        "    y_pred = self.predict(X)\n",
        "    \n",
        "    # Convert y_true to one hot\n",
        "    y_true = np.stack([y_true, 1-y_true], axis=-1)\n",
        "    loss = np.mean(-y_true * np.log(y_pred))\n",
        "    \n",
        "    return loss\n",
        "    \n",
        "def fit(self, X_train, y_train, n_epochs, batch_size=1, learning_rate=1e-5):\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        n_batches = np.int(np.floor(X_train.shape[0] / batch_size))\n",
        "        \n",
        "        # Generate random index\n",
        "        index = np.arange(X_train.shape[0])\n",
        "        np.random.shuffle(index)\n",
        "        \n",
        "        # Iterate over batches\n",
        "        loss_list = []\n",
        "        for batch in range(n_batches):\n",
        "            beg = batch*batch_size\n",
        "            end = (batch+1)*batch_size if (batch+1)*batch_size < X_train.shape[0] else -1\n",
        "            X_batch = X_train[beg:end]\n",
        "            y_batch = y_train[beg:end]\n",
        "            \n",
        "            # Compute the loss\n",
        "            loss = self.loss(X_batch, y_batch)\n",
        "            loss_list.append(loss)\n",
        "            \n",
        "            # Compute the gradient\n",
        "            gradient = self.grad(X_batch, y_batch)\n",
        "            \n",
        "            # Compute the mean gradient over all the example images\n",
        "            gradient = np.mean(gradient, axis=0, keepdims=False)\n",
        "\n",
        "            # Update the weights\n",
        "            self.W -= learning_rate * gradient\n",
        "            \n",
        "        return loss_list\n",
        "\n",
        "# Assign methods to class\n",
        "setattr(LinearClassifier, 'grad', grad)\n",
        "setattr(LinearClassifier, 'loss', loss)\n",
        "setattr(LinearClassifier, 'fit', fit)\n",
        "\n",
        "lc = LinearClassifier()\n",
        "loss = lc.fit(X_train, y_train, n_epochs=10, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh-GWNb-3x1m"
      },
      "source": [
        "## Evaluate the model\n",
        "Benchmarking performance is a critical part of the model development process. For this problem, we will use 3 different benchmarks\n",
        "- Recall: the fraction of positive examples detected by a model. Mathematically, for a two-class classification problem, recall is calculated as (True positives)/(True positives + False negatives). \n",
        "- Precision: the percentage of positive predictions from a model that are true. Mathematically, for a two-class prediction problem, precision is calculated as (True positives)/(True positives + False positives).\n",
        "- F1 score: The harmonic mean between the recall and precision\n",
        "\n",
        "We will evaluate these metrics on both the training dataset (the examples used during training) and our testing dataset (the examples that we held out). We can also use a confusion matrix to visualize the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_wsv4zW3x1m"
      },
      "outputs": [],
      "source": [
        "# Visualize some predictions\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(20,20))\n",
        "for i in range(8):\n",
        "    \n",
        "    # Get an example image\n",
        "    X_sample = X_test[[i],...]\n",
        "    \n",
        "    # Reshape flattened vector to image\n",
        "    X_reshape = np.reshape(X_sample, (128,128))\n",
        "    \n",
        "    # Predict the label\n",
        "    y_pred = lc.predict(X_sample)\n",
        "    \n",
        "    # Display results\n",
        "    axes.flatten()[i].imshow(X_reshape, cmap='gray')\n",
        "    axes.flatten()[i].set_title('Label ' + str(y[i]) +', Prediction ' + str(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWgRBoPG3x1m"
      },
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "y_pred = lc.predict(X_train)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "# Compute metrics\n",
        "recall = sklearn.metrics.recall_score(y_train, y_pred)\n",
        "precision = sklearn.metrics.precision_score(y_train, y_pred)\n",
        "f1 = sklearn.metrics.f1_score(y_train, y_pred)\n",
        "\n",
        "print('Training Recall: {}'.format(recall))\n",
        "print('Training Precision: {}'.format(precision))\n",
        "print('Training F1 Score: {}'.format(f1))\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = lc.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "# Compute metrics\n",
        "recall = sklearn.metrics.recall_score(y_test, y_pred)\n",
        "precision = sklearn.metrics.precision_score(y_test, y_pred)\n",
        "f1 = sklearn.metrics.f1_score(y_test, y_pred)\n",
        "\n",
        "print('Testing Recall: {}'.format(recall))\n",
        "print('Testing Precision: {}'.format(precision))\n",
        "print('Testing F1 Score: {}'.format(f1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQndjjYp3x1m"
      },
      "source": [
        "## Exercise\n",
        "Try running your training algorithm a few times and record the results. What do you note about the overall performance? What about the differences between training runs? What about the difference in performance when evaluated on training data as opposed to validation data?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "linear-classifier-key.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
